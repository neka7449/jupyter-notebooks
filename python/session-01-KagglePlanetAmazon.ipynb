{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat ../../.keras/keras.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras import applications\n",
    "\n",
    "from sklearn.metrics import fbeta_score\n",
    "# from sklearn.metrics import matthews_corrcoef, hamming_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = \"./results/session-01-KagglePlanetAmazon/\"\n",
    "DATA_DIR = \"../../Data/kaggle-planet-amazon/\"\n",
    "\n",
    "# !head ../../Data/kaggle-planet-amazon/train_v2.csv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv(DATA_DIR + \"train_v2.csv\")\n",
    "\n",
    "# df_train[\"tags\"] = df_train[\"tags\"].map(lambda x: x.split(\" \"))\n",
    "\n",
    "# df_train.head()\n",
    "\n",
    "# binarizedLabels_df = pd.DataFrame(binarizedLabels)\n",
    "# binarizedLabels_df.columns = mlb.classes_\n",
    "# binarizedLabels_df.head()\n",
    "\n",
    "# df_valid_samples = df_train.sample(frac=0.2)\n",
    "\n",
    "# df_train_samples = df_train.drop(df_valid_samples.index)\n",
    "\n",
    "# def PrintShape(df, df_shape):\n",
    "#     print(\"{} shape: {}\".format(df, df_shape))\n",
    "\n",
    "# PrintShape('df_train_samples', df_train_samples.shape)\n",
    "# PrintShape('df_valid_samples', df_valid_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# mlb = MultiLabelBinarizer()\n",
    "\n",
    "# y_train = mlb.fit_transform(df_train_samples[\"tags\"])\n",
    "# y_valid = mlb.fit_transform(df_valid_samples[\"tags\"])\n",
    "\n",
    "# mlb.classes_\n",
    "\n",
    "####################################\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "labels = list(set(flatten([l.split(' ') for l in df_train['tags'].values])))\n",
    "\n",
    "label_map = {l: i for i, l in enumerate(labels)}\n",
    "inv_label_map = {i: l for l, i in label_map.items()}\n",
    "\n",
    "# Params\n",
    "input_size = 64\n",
    "input_channels = 3\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "lr_decay = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_valid = []\n",
    "# y_valid = []\n",
    "\n",
    "# for f, tags in tqdm(df_valid_samples.values, miniters=1000):\n",
    "#     img = cv2.resize(cv2.imread(DATA_DIR + 'train-jpg/{}.jpg'.format(f)), (input_size, input_size))\n",
    "#     targets = np.zeros(17)\n",
    "#     for t in tags.split(' '):\n",
    "#         targets[label_map[t]] = 1\n",
    "#     x_valid.append(img)\n",
    "#     y_valid.append(targets)\n",
    "\n",
    "# y_valid = np.array(y_valid, np.uint8)\n",
    "# x_valid = np.array(x_valid, np.float32)\n",
    "\n",
    "# x_train = []\n",
    "# y_train = []\n",
    "\n",
    "# for f, tags in tqdm(df_train_samples.values, miniters=1000):\n",
    "#     img = cv2.resize(cv2.imread(DATA_DIR + 'train-jpg/{}.jpg'.format(f)), (input_size, input_size))\n",
    "#     targets = np.zeros(17)\n",
    "#     for t in tags.split(' '):\n",
    "#         targets[label_map[t]] = 1\n",
    "#     x_train.append(img)\n",
    "#     y_train.append(targets)\n",
    "#     img = cv2.flip(img, 0)  # flip vertically\n",
    "#     x_train.append(img)\n",
    "#     y_train.append(targets)\n",
    "#     img = cv2.flip(img, 1)  # flip horizontally\n",
    "#     x_train.append(img)\n",
    "#     y_train.append(targets)\n",
    "#     img = cv2.flip(img, 0)  # flip vertically\n",
    "#     x_train.append(img)\n",
    "#     y_train.append(targets)\n",
    "\n",
    "# y_train = np.array(y_train, np.uint8)\n",
    "# x_train = np.array(x_train, np.float32)\n",
    "\n",
    "df_test_samples = pd.read_csv(DATA_DIR + 'sample_submission_v2.csv')\n",
    "\n",
    "# x_test = []\n",
    "\n",
    "# for f, tags in tqdm(df_test_samples.values, miniters=1000):\n",
    "#     img = cv2.resize(cv2.imread(DATA_DIR + 'test-jpg/{}.jpg'.format(f)), (input_size, input_size))\n",
    "#     x_test.append(img)\n",
    "\n",
    "# x_test = np.array(x_test, np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# with open('image_vars.pkl', 'wb') as opfile:\n",
    "#     pickle.dump(y_valid, opfile, protocol=4)\n",
    "#     pickle.dump(x_valid, opfile, protocol=4)\n",
    "#     pickle.dump(y_train, opfile, protocol=4)\n",
    "#     pickle.dump(x_train, opfile, protocol=4)\n",
    "#     pickle.dump(x_test, opfile, protocol=4)\n",
    "\n",
    "y_valid = []\n",
    "x_valid = []\n",
    "y_train = []\n",
    "x_train = []\n",
    "x_test = []\n",
    "\n",
    "with open('image_vars.pkl', 'rb') as ipfile:\n",
    "    y_valid = pickle.load(ipfile)\n",
    "    x_valid = pickle.load(ipfile)\n",
    "    y_train = pickle.load(ipfile)\n",
    "    x_train = pickle.load(ipfile)\n",
    "    x_test = pickle.load(ipfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using functional API\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# This returns a tensor\n",
    "inputs = Input(shape=(input_size, input_size, input_channels))\n",
    "\n",
    "# a layer instance is callable on a tensor, and returns a tensor\n",
    "x = BatchNormalization()(inputs)\n",
    "x = Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu')(x)\n",
    "x = Conv2D(32, kernel_size=(3, 3), activation='relu') (x)\n",
    "x = MaxPooling2D(pool_size=(3, 3))(x)\n",
    "x = Dropout(0.25)(x)\n",
    "\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu')(x)\n",
    "x = Conv2D(64, kernel_size=(3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size=(3, 3))(x)\n",
    "x = Dropout(0.25)(x)\n",
    "\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, kernel_size=(3, 3), padding='same', activation='relu')(x)\n",
    "x = Conv2D(128, kernel_size=(3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(0.25)(x)\n",
    "\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu')(x)\n",
    "x = Conv2D(256, kernel_size=(2, 2), activation='relu')(x)\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "x = Dropout(0.25)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(17, activation='sigmoid')(x)\n",
    "\n",
    "# This creates a model that includes\n",
    "# the Input layer and three Dense layers\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # using sequential API\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(BatchNormalization(input_shape=(input_size, input_size, input_channels)))\n",
    "# model.add(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "# model.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "# model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Conv2D(128, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "# model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "# model.add(Conv2D(256, kernel_size=(2, 2), activation='relu'))\n",
    "# # model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(512, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(17, activation='sigmoid'))\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss',\n",
    "                           patience=5,\n",
    "                           verbose=0),\n",
    "             TensorBoard(log_dir='logs'),\n",
    "             ModelCheckpoint('weights.h5',\n",
    "                             save_best_only=True)]\n",
    "\n",
    "opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              # We NEED binary here, since categorical_crossentropy l1 norms the output before calculating loss.\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=2,\n",
    "          callbacks=callbacks,\n",
    "          validation_data=(x_valid, y_valid))  # starts training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_test = model.predict(x_test, batch_size=batch_size, verbose=2)\n",
    "p_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_valid = model.predict(x_valid, batch_size=batch_size)\n",
    "p_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_valid = model.predict(x_valid, batch_size=batch_size)\n",
    "print(fbeta_score(y_valid, np.array(p_valid) >= 0.2, beta=2, average='samples'))\n",
    "\n",
    "y_test = []\n",
    "\n",
    "p_test = model.predict(x_test, batch_size=batch_size, verbose=2)\n",
    "y_test.append(p_test)\n",
    "\n",
    "result = np.array(y_test[0])\n",
    "result = pd.DataFrame(result, columns=labels)\n",
    "\n",
    "preds = []\n",
    "\n",
    "for i in tqdm(range(result.shape[0]), miniters=1000):\n",
    "    a = result.ix[[i]]\n",
    "    a = a.apply(lambda x: x >= 0.2, axis=1)\n",
    "    a = a.transpose()\n",
    "    a = a.loc[a[i] == True]\n",
    "    ' '.join(list(a.index))\n",
    "    preds.append(' '.join(list(a.index)))\n",
    "\n",
    "df_test_samples['tags'] = preds\n",
    "df_test_samples.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!head submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "submission_samples = pd.read_csv('./submission.csv')\n",
    "for f, tags in tqdm(df_valid_samples[:50].values, miniters=1000):\n",
    "    display(Image(filename=DATA_DIR + 'train-jpg/{}.jpg'.format(f)))\n",
    "    print(tags)\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "threshold = np.arange(0.1,0.9,0.1)\n",
    "\n",
    "acc = []\n",
    "accuracies = []\n",
    "best_threshold = np.zeros(p_valid.shape[1])\n",
    "for i in range(p_valid.shape[1]):\n",
    "    y_prob = np.array(p_valid[:,i])\n",
    "    for j in threshold:\n",
    "        y_pred = [1 if prob>=j else 0 for prob in y_prob]\n",
    "        acc.append(matthews_corrcoef(y_valid[:,i],y_pred))\n",
    "    acc   = np.array(acc)\n",
    "    index = np.where(acc==acc.max()) \n",
    "    accuracies.append(acc.max()) \n",
    "    best_threshold[i] = threshold[index[0][0]]\n",
    "    acc = []\n",
    "    \n",
    "best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_valid = model.predict(x_valid, batch_size=batch_size)\n",
    "print(fbeta_score(y_valid, np.array(p_valid) >= best_threshold, beta=2, average='samples'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# best_threshold\n",
    "\n",
    "# p_test.shape[1]\n",
    "\n",
    "# y_pred = np.array([[1 if p_test[i,j]>=best_threshold[j] else 0 for j in range(p_test.shape[1])] for i in range(p_test.shape[0])])\n",
    "\n",
    "# y_pred\n",
    "\n",
    "# df_test_samples['tags'] = y_pred\n",
    "# df_test_samples.to_csv('submission.csv', index=False)\n",
    "\n",
    "# p_valid = model.predict(x_valid, batch_size=batch_size)\n",
    "# print(fbeta_score(y_valid, np.array(p_valid) >= 0.2, beta=2, average='samples'))\n",
    "\n",
    "# y_test = []\n",
    "\n",
    "# p_test = model.predict(x_test, batch_size=batch_size, verbose=2)\n",
    "# y_test.append(p_test)\n",
    "\n",
    "# result = np.array(y_test[0])\n",
    "# result = pd.DataFrame(result, columns=labels)\n",
    "\n",
    "# preds = []\n",
    "\n",
    "# for i in tqdm(range(result.shape[0]), miniters=1000):\n",
    "#     a = result.ix[[i]]\n",
    "#     a = a.apply(lambda x: x >= 0.3, axis=1)\n",
    "#     a = a.transpose()\n",
    "#     a = a.loc[a[i] == True]\n",
    "#     ' '.join(list(a.index))\n",
    "#     preds.append(' '.join(list(a.index)))\n",
    "\n",
    "# df_test_samples['tags'] = preds\n",
    "# df_test_samples.to_csv('submission.csv', index=False)\n",
    "\n",
    "# !head submission.csv\n",
    "\n",
    "# p_valid = model.predict(x_valid, batch_size=batch_size)\n",
    "# print(fbeta_score(y_valid, np.array(p_valid) >= 0.3, beta=2, average='samples'))\n",
    "\n",
    "# y_test = np.array(y_test, np.uint8)\n",
    "\n",
    "# y_test\n",
    "\n",
    "# p_valid\n",
    "\n",
    "# p_test\n",
    "\n",
    "# from sklearn.metrics import matthews_corrcoef\n",
    "# from sklearn.metrics import hamming_loss\n",
    "\n",
    "# threshold = np.arange(0.1,0.9,0.1)\n",
    "\n",
    "# acc = []\n",
    "# accuracies = []\n",
    "# best_threshold = np.zeros(p_valid.shape[1])\n",
    "# for i in range(p_valid.shape[1]):\n",
    "#     y_prob = np.array(p_valid[:,i])\n",
    "#     for j in threshold:\n",
    "#         y_pred = [1 if prob>=j else 0 for prob in y_prob]\n",
    "#         acc.append(matthews_corrcoef(y_valid[:,i],y_pred))\n",
    "#     acc   = np.array(acc)\n",
    "#     index = np.where(acc==acc.max()) \n",
    "#     accuracies.append(acc.max()) \n",
    "#     best_threshold[i] = threshold[index[0][0]]\n",
    "#     acc = []\n",
    "\n",
    "# best_threshold\n",
    "\n",
    "# print(fbeta_score(y_valid, np.array(p_valid) > 0.4, beta=2, average='samples'))\n",
    "\n",
    "# len(y_test[0])\n",
    "\n",
    "# y_pred = np.array([[1 if p_valid[i,j]>=best_threshold[j] else 0 for j in range(y_valid.shape[1])] for i in range(len(y_test[0]))])\n",
    "\n",
    "# hamming_loss(y_valid,y_pred)\n",
    "\n",
    "# total_correctly_predicted = len([i for i in range(len(y_valid)) if (y_valid[i]==y_pred[i]).sum() == 17])\n",
    "\n",
    "# total_correctly_predicted\n",
    "\n",
    "# x_valid.shape\n",
    "\n",
    "# (total_correctly_predicted/ x_valid.shape[0]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_bottleneck_features():\n",
    "    model = applications.VGG16(weights = 'imagenet', include_top = False)\n",
    "\n",
    "    datagen = ImageDataGenerator(\n",
    "                    rescale=1./255,\n",
    "                    rotation_range = 40,\n",
    "                    width_shift_range = 0.2,\n",
    "                    height_shift_range = 0.2,\n",
    "                    shear_range = 0.2,\n",
    "                    zoom_range = 0.2,\n",
    "                    fill_mode = 'nearest'\n",
    "                    )\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "                            '../../Data/kaggle-planet-amazon/train-jpg/',\n",
    "                            target_size = (64, 64),\n",
    "                            batch_size = 128,\n",
    "                            class_mode = None,\n",
    "                            shuffle = False\n",
    "                            )\n",
    "\n",
    "    bottleneck_features_train = model.predict_generator(generator, len(train_x)//batch_size)\n",
    "    np.save(open('bottleneck_features_train.npy','wb'), bottleneck_features_train)\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "                            './data/validation',\n",
    "                            target_size = (img_width, img_height),\n",
    "                            batch_size = batch_size,\n",
    "                            class_mode = None,\n",
    "                            shuffle = False\n",
    "                            )\n",
    "    bottleneck_features_val = model.predict_generator(generator, len(val_x)//batch_size)\n",
    "    np.save(open('bottleneck_features_val.npy','wb'), bottleneck_features_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Current Mean F2-Score: 0.58"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
