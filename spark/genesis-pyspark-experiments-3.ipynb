{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.7.56.175:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://localhost:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f3a197ce390>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "import numpy as np\n",
    "from dataset import dataset\n",
    "from dummyEncoder import dummyEncoder\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 2:\n",
    "        print(\"Usage: wordcount <file>\", file=sys.stderr)\n",
    "        exit(-1)\n",
    "\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"PythonWordCount\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    sampleData = 'data/krk/krk'\n",
    "    trainFilename = sampleData + '.data'\n",
    "    namesFilename = sampleData + '.names'\n",
    "\n",
    "#     import gc\n",
    "#     import numpy as np\n",
    "#     from dataset import dataset\n",
    "#     # from options import getOptions\n",
    "#     # from learningModel import learningModel\n",
    "#     from dummyEncoder import dummyEncoder\n",
    "\n",
    "    trainingSet = dataset(trainFilename, namesFilename)\n",
    "    trainingSet.data.head()\n",
    "\n",
    "    from pyspark.ml.linalg import Vectors\n",
    "\n",
    "    de = dummyEncoder()\n",
    "    temp = de.fit_transform(trainingSet.data)\n",
    "    temp = map(lambda arr: Vectors.dense(arr), temp)\n",
    "    data = spark.createDataFrame(zip(trainingSet.classes, temp),schema=[\"label\", \"features\"])\n",
    "\n",
    "    (trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "#     from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "#     from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "#     from pyspark.ml.classification import RandomForestClassifier\n",
    "#     from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "#     from pyspark.ml import Pipeline\n",
    "\n",
    "    featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\").fit(data)\n",
    "\n",
    "    labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n",
    "\n",
    "    rf = RandomForestClassifier(labelCol=\"indexedLabel\", \\\n",
    "                                featuresCol=\"features\", \\\n",
    "                                numTrees=30, \\\n",
    "                                featureSubsetStrategy='onethird',\n",
    "                                maxDepth=30,\n",
    "                                impurity='gini')\n",
    "\n",
    "    labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                                   labels=labelIndexer.labels)\n",
    "\n",
    "    pipeline = Pipeline(stages=[featureIndexer, labelIndexer, rf, labelConverter])\n",
    "\n",
    "    # Select (prediction, true label) and compute test error\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "    paramGrid = ParamGridBuilder().build()\n",
    "\n",
    "    crossval = CrossValidator(\n",
    "        estimator=pipeline,\n",
    "        estimatorParamMaps=paramGrid,\n",
    "        evaluator=evaluator,\n",
    "        numFolds=5)\n",
    "\n",
    "    # Train model.  This also runs the indexers.\n",
    "    model = crossval.fit(trainingData)\n",
    "\n",
    "    # Make predictions.\n",
    "    predictions = model.transform(testData)\n",
    "\n",
    "    predictions.show()\n",
    "\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "    \n",
    "    spark.stop()\n",
    "\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !ls -al\n",
    "\n",
    "# import numpy as np\n",
    "# from dataset import dataset\n",
    "# from options import getOptions\n",
    "# from learningModel import learningModel\n",
    "# from dummyEncoder import dummyEncoder\n",
    "\n",
    "# trainingSet = dataset(trainFilename, namesFilename)\n",
    "\n",
    "# trainingSet.data.head()\n",
    "\n",
    "# import pickle\n",
    "# import gzip\n",
    "# import sys\n",
    "# import numpy as np\n",
    "# from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor\n",
    "# from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "# from sklearn.ensemble import BaggingClassifier, BaggingRegressor\n",
    "# from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "# from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor\n",
    "# from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "# from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "# from sklearn.calibration import calibration_curve\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.svm import SVC, SVR\n",
    "# from sklearn.svm import NuSVC, NuSVR\n",
    "# from sklearn.svm import LinearSVC, LinearSVR\n",
    "# from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "# from sklearn.model_selection import StratifiedKFold, LeaveOneOut, KFold\n",
    "# from sklearn.metrics import confusion_matrix, accuracy_score, explained_variance_score\n",
    "# from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "# from sklearn.metrics import recall_score, precision_score, f1_score, log_loss\n",
    "# from sklearn.preprocessing import Imputer\n",
    "# # from xgboost import XGBClassifier\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from dummyEncoder import dummyEncoder\n",
    "\n",
    "# classifier = GradientBoostingClassifier(n_estimators=1, \\\n",
    "# #                                    nthread=-1, \\\n",
    "#                                    random_state=None, \\\n",
    "#                                    max_depth=None)\n",
    "\n",
    "# model = Pipeline([('data', dummyEncoder()), \\\n",
    "#                   ('imputer', Imputer(missing_values='NaN', strategy='median', axis=0, verbose=100)), \\\n",
    "#                   ('classifier', classifier)])\n",
    "\n",
    "# kFold = KFold(n_splits=2, shuffle=True, random_state=None)\n",
    "# groundTruth = trainingSet.classes\n",
    "# datasetPredictions = [None] * len(groundTruth)\n",
    "# for trainIndices, testIndices in kFold.split(trainingSet.data, trainingSet.classes):\n",
    "#     trainX = trainingSet.data.ix[trainIndices, :]\n",
    "#     trainY = trainingSet.classes[trainIndices]\n",
    "#     testX = trainingSet.data.ix[testIndices, :]\n",
    "#     testY = trainingSet.classes[testIndices]\n",
    "\n",
    "#     # Learn the model\n",
    "#     model.fit(trainX, trainY)\n",
    "    \n",
    "#     foldPredictions = model.predict(testX)\n",
    "    \n",
    "#     for i, v in enumerate(testIndices):\n",
    "#         datasetPredictions[v] = foldPredictions[i]\n",
    "    \n",
    "\n",
    "# import pandas as pd\n",
    "# new_df = pd.concat((trainingSet.data, trainingSet.classes), axis=1)\n",
    "# new_df.head()\n",
    "\n",
    "# type(trainingSet.classes)\n",
    "\n",
    "# from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# de = dummyEncoder()\n",
    "# temp = de.fit_transform(trainingSet.data)\n",
    "# temp = map(lambda arr: Vectors.dense(arr), temp)\n",
    "# data = spark.createDataFrame(zip(trainingSet.classes, temp),schema=[\"label\", \"features\"])\n",
    "\n",
    "# # data.show()\n",
    "\n",
    "# # mydf.show()\n",
    "# from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "# from pyspark.ml.classification import RandomForestClassifier\n",
    "# from pyspark.ml import Pipeline\n",
    "\n",
    "# labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n",
    "# featureIndexer =\\\n",
    "#     VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\").fit(data)\n",
    "# (trainingData, testData) = data.randomSplit([0.8, 0.2])\n",
    "# rf = RandomForestClassifier(labelCol=\"indexedLabel\", \\\n",
    "#                             featuresCol=\"features\", \\\n",
    "#                             numTrees=30, \\\n",
    "#                             featureSubsetStrategy='onethird',\n",
    "#                             maxDepth=12,\n",
    "#                             impurity='entropy')\n",
    "# labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "#                                labels=labelIndexer.labels)\n",
    "\n",
    "# pipeline = Pipeline(stages=[featureIndexer, labelIndexer, rf, labelConverter])\n",
    "# # pipeline = Pipeline(stages=[featureIndexer, rf])\n",
    "\n",
    "\n",
    "# # Train model.  This also runs the indexers.\n",
    "# model = pipeline.fit(trainingData)\n",
    "\n",
    "# # Make predictions.\n",
    "# predictions = model.transform(testData)\n",
    "\n",
    "# # Select example rows to display.\n",
    "# # predictions.select(\"predictedLabel\", \"label\", \"features\").show(5)\n",
    "\n",
    "# # Select (prediction, true label) and compute test error\n",
    "# evaluator = MulticlassClassificationEvaluator(\n",
    "#     labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "# accuracy = evaluator.evaluate(predictions)\n",
    "# print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "\n",
    "\n",
    "# from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# rf = RandomForestClassifier(labelCol=\"indexed\", featuresCol=\"features\", numTrees=100)\n",
    "\n",
    "# from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "# labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "#                                labels=labelIndexer.labels)\n",
    "# model = rf.fit()\n",
    "\n",
    "# [type(ii) for ii in temp]\n",
    "\n",
    "# from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# # dff = map(lambda x: Vectors.dense(x[:]), temp)\n",
    "# dff = spark.sparkContext.parallelize(temp)\n",
    "# dff.take(5)\n",
    "# dff = dff.map(lambda x: )\n",
    "# # dff.first() == temp[0]\n",
    "# mydf = spark.createDataFrame(dff,schema=[\"features\"])\n",
    "\n",
    "# from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "# from pyspark.mllib.util import MLUtils\n",
    "\n",
    "# # Load and parse the data file into an RDD of LabeledPoint.\n",
    "# data = MLUtils.loadLibSVMFile(sc, '../../jupyter-notebooks/datasets/sample_libsvm_data.txt')\n",
    "# # Split the data into training and test sets (30% held out for testing)\n",
    "# (trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "\n",
    "# testData.map(lambda x: x.label).take(5)\n",
    "\n",
    "# f = urllib.request.urlretrieve (\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data.gz\", \"kddcup.data.gz\")\n",
    "# data_file = \"./kddcup.data.gz\"\n",
    "# raw_data = sc.textFile(data_file)\n",
    "# print(\"Train data size is {}\".format(raw_data.count()))\n",
    "# ft = urllib.request.urlretrieve(\"http://kdd.ics.uci.edu/databases/kddcup99/corrected.gz\", \"corrected.gz\")\n",
    "# test_data_file = \"./corrected.gz\"\n",
    "# test_raw_data = sc.textFile(test_data_file)\n",
    "# print (\"Test data size is {}\".format(test_raw_data.count()))\n",
    "\n",
    "# from pyspark.ml.feature import VectorIndexer\n",
    "\n",
    "# data = spark.read.format(\"libsvm\").load(\"../../jupyter-notebooks/datasets/sample_libsvm_data.txt\")\n",
    "\n",
    "# indexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexed\", maxCategories=10)\n",
    "# indexerModel = indexer.fit(data)\n",
    "\n",
    "# categoricalFeatures = indexerModel.categoryMaps\n",
    "# print(\"Chose %d categorical features: %s\" %\n",
    "#       (len(categoricalFeatures), \", \".join(str(k) for k in categoricalFeatures.keys())))\n",
    "\n",
    "# # Create new column \"indexed\" with categorical values transformed to indices\n",
    "# indexedData = indexerModel.transform(data)\n",
    "# indexedData.show()\n",
    "\n",
    "# gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
