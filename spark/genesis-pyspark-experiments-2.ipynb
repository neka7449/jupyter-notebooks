{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.\\\n",
    "option(\"head\", None).\\\n",
    "option(\"inferSchema\", True).\\\n",
    "csv(\"../Documents/src/flask-genesis/genesis/data/krk/krk.data\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: integer (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: integer (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: integer (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c1', '_c3', '_c5']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[c[0] for c in df[df.columns].dtypes if c[1] == 'int']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_indexer_input(data):\n",
    "    str_cols_value = {}\n",
    "    for c, t in data[data.columns].dtypes:\n",
    "        if t == 'string':\n",
    "            str_cols_value[c] = StringIndexer(inputCol=c, outputCol='indexed_' + c).fit(data)\n",
    "    return str_cols_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "\n",
    "indexer_input = get_indexer_input(df)\n",
    "data_test, data_train = df.randomSplit(weights=[0.3, 0.7], seed=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['_c0', '_c2', '_c4', '_c6'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexer_input.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = list(set(data_train.columns) - set(indexer_input.keys())) + str_ind_cols - ['indexed__c6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_ind_cols = ['indexed_' + column for column in indexer_input.keys() if column != '_c6']\n",
    "x_cols = list(set(data_train.columns) - set(indexer_input.keys())) + str_ind_cols\n",
    "print (x_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_c5', '_c3', '_c1', 'indexed__c0', 'indexed__c2', 'indexed__c4']\n",
      "+---+---+---+-----------+-----------+-----------+-----------+\n",
      "|_c1|_c3|_c5|indexed__c0|indexed__c2|indexed__c4|indexed__c6|\n",
      "+---+---+---+-----------+-----------+-----------+-----------+\n",
      "|  1|  2|  1|        3.0|        4.0|        7.0|        9.0|\n",
      "|  1|  2|  2|        3.0|        4.0|        7.0|        5.0|\n",
      "|  1|  2|  1|        3.0|        4.0|        5.0|        8.0|\n",
      "|  1|  2|  3|        3.0|        4.0|        5.0|        5.0|\n",
      "|  1|  2|  1|        3.0|        4.0|        3.0|        9.0|\n",
      "|  1|  2|  2|        3.0|        4.0|        3.0|        0.0|\n",
      "|  1|  2|  4|        3.0|        4.0|        3.0|        5.0|\n",
      "|  1|  2|  1|        3.0|        4.0|        2.0|        8.0|\n",
      "|  1|  2|  2|        3.0|        4.0|        2.0|        1.0|\n",
      "|  1|  2|  3|        3.0|        4.0|        2.0|        5.0|\n",
      "|  1|  2|  4|        3.0|        4.0|        2.0|        5.0|\n",
      "|  1|  2|  5|        3.0|        4.0|        2.0|        5.0|\n",
      "|  1|  2|  1|        3.0|        4.0|        1.0|        9.0|\n",
      "|  1|  2|  3|        3.0|        4.0|        1.0|        5.0|\n",
      "|  1|  2|  1|        3.0|        4.0|        0.0|        8.0|\n",
      "|  1|  2|  2|        3.0|        4.0|        0.0|        2.0|\n",
      "|  1|  2|  3|        3.0|        4.0|        0.0|        0.0|\n",
      "|  1|  2|  4|        3.0|        4.0|        0.0|        0.0|\n",
      "|  1|  2|  7|        3.0|        4.0|        0.0|        0.0|\n",
      "|  1|  3|  1|        3.0|        4.0|        7.0|        3.0|\n",
      "+---+---+---+-----------+-----------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.tree import RandomForest\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "str_ind_cols = ['indexed_' + column for column in indexer_input.keys() if column != '_c6']\n",
    "x_cols = list(set(data_train.columns) - set(indexer_input.keys())) + str_ind_cols\n",
    "print (x_cols)\n",
    "# x_cols.append('_c6')\n",
    "indexers = indexer_input.values()\n",
    "pipeline_tr = Pipeline(stages=indexers)\n",
    "data_tr = pipeline_tr.fit(data_train).transform(data_train)\n",
    "data_tr = data_tr.drop('_c0', '_c2', '_c4', '_c6')\n",
    "data_tr.show()\n",
    "assembler = VectorAssembler(inputCols=x_cols, outputCol=\"features\")\n",
    "# rfc = RandomForest.trainClassifier(trainingData, numClasses=18, categoricalFeaturesInfo={},\n",
    "#                                      numTrees=3, featureSubsetStrategy=\"auto\",\n",
    "#                                      impurity='gini', maxDepth=4, maxBins=32)\n",
    "# gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"indexed__c6\", stepSize=0.008, maxDepth=5, subsamplingRate=0.75,\n",
    "#                        seed=10, maxIter=500, minInstancesPerNode=5, checkpointInterval=100, maxBins=64)\n",
    "# pipeline_training = Pipeline(stages=[assembler, gbt])\n",
    "# model = pipeline_training.fit(data_tr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c1', 'int'),\n",
       " ('_c3', 'int'),\n",
       " ('_c5', 'int'),\n",
       " ('indexed__c0', 'double'),\n",
       " ('indexed__c2', 'double'),\n",
       " ('indexed__c4', 'double'),\n",
       " ('indexed__c6', 'double')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tr[data_tr.columns].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data_tr = data_tr.select(*(col(c).cast(\"float\").alias(c) if c in [d[0] for d in data_tr[data_tr.columns].dtypes if d[1] == 'int'] else col(c) for c in data_tr.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-----------+-----------+-----------+-----------+\n",
      "|_c1|_c3|_c5|indexed__c0|indexed__c2|indexed__c4|indexed__c6|\n",
      "+---+---+---+-----------+-----------+-----------+-----------+\n",
      "|1.0|2.0|1.0|        3.0|        4.0|        7.0|        9.0|\n",
      "|1.0|2.0|2.0|        3.0|        4.0|        7.0|        5.0|\n",
      "|1.0|2.0|1.0|        3.0|        4.0|        5.0|        8.0|\n",
      "|1.0|2.0|3.0|        3.0|        4.0|        5.0|        5.0|\n",
      "|1.0|2.0|1.0|        3.0|        4.0|        3.0|        9.0|\n",
      "|1.0|2.0|2.0|        3.0|        4.0|        3.0|        0.0|\n",
      "|1.0|2.0|4.0|        3.0|        4.0|        3.0|        5.0|\n",
      "|1.0|2.0|1.0|        3.0|        4.0|        2.0|        8.0|\n",
      "|1.0|2.0|2.0|        3.0|        4.0|        2.0|        1.0|\n",
      "|1.0|2.0|3.0|        3.0|        4.0|        2.0|        5.0|\n",
      "|1.0|2.0|4.0|        3.0|        4.0|        2.0|        5.0|\n",
      "|1.0|2.0|5.0|        3.0|        4.0|        2.0|        5.0|\n",
      "|1.0|2.0|1.0|        3.0|        4.0|        1.0|        9.0|\n",
      "|1.0|2.0|3.0|        3.0|        4.0|        1.0|        5.0|\n",
      "|1.0|2.0|1.0|        3.0|        4.0|        0.0|        8.0|\n",
      "|1.0|2.0|2.0|        3.0|        4.0|        0.0|        2.0|\n",
      "|1.0|2.0|3.0|        3.0|        4.0|        0.0|        0.0|\n",
      "|1.0|2.0|4.0|        3.0|        4.0|        0.0|        0.0|\n",
      "|1.0|2.0|7.0|        3.0|        4.0|        0.0|        0.0|\n",
      "|1.0|3.0|1.0|        3.0|        4.0|        7.0|        3.0|\n",
      "+---+---+---+-----------+-----------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_data_tr.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.mllib.linalg import Vector as MLLibVector, Vectors as MLLibVectors\n",
    "from pyspark.ml.linalg import Vector as MLVector, Vectors as MLVectors\n",
    "\n",
    "\n",
    "transformed = assembler.transform(temp_data_tr)\n",
    "lp_transformed = transformed.select(col(\"indexed__c6\").alias(\"label\"), col(\"features\"))\\\n",
    ".rdd\\\n",
    ".map(lambda x: LabeledPoint(x.label, as_mllib(x.features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(9.0, [1.0,2.0,1.0,3.0,4.0,7.0]),\n",
       " LabeledPoint(5.0, [2.0,2.0,1.0,3.0,4.0,7.0])]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lp_transformed.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import linalg as ml_linalg\n",
    "\n",
    "def as_mllib(v):\n",
    "    if isinstance(v, ml_linalg.SparseVector):\n",
    "        return MLLibVectors.sparse(v.size, v.indices, v.values)\n",
    "    elif isinstance(v, ml_linalg.DenseVector):\n",
    "        return MLLibVectors.dense(v.toArray())\n",
    "    else:\n",
    "        raise TypeError(\"Unsupported type: {0}\".format(type(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
