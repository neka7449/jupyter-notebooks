{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.7.56.175:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f27049059b0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load ../src/genesis/dataset.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from names import namesFileParser\n",
    "\n",
    "class dataset(object):\n",
    "    \"\"\"Representation of a dataset\n",
    "\n",
    "    Each row contains a feature vector.  Continuous and discrete features are\n",
    "    handled within the vector.  The class labels may also be discrete or \n",
    "    continuous, representative of classification or regression, respectively.\n",
    "\n",
    "    The dataset is read in by processing a CSV file and a .names file\n",
    "    describing the CSV file contains.  Alternatively, manual specification of\n",
    "    the .names file contents through arguments to __init__ may be provided.\n",
    "\n",
    "    Feature vectors containing missing data is also handled.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataFilename, namesFilename=None, attributeNames=None, classNames=None):\n",
    "\n",
    "        self.hasClassLabels = False  # Not all datasets must have class labels (test set)\n",
    "        self.attributeNames = attributeNames  # List of attribute names\n",
    "        self.classNames = classNames  # List of class names\n",
    "        self.classes = None  # Dataframe of class labels\n",
    "        self.data = None  # Dataframe of dataset\n",
    "\n",
    "        # Determine if this is a regression problem\n",
    "        self.regression = namesFileParser.isRegression(self.classNames)\n",
    "\n",
    "        self.readData(dataFilename, namesFilename)\n",
    "\n",
    "    def readData(self, dataFilename, namesFilename):\n",
    "        \"\"\"Loads a dataset into memory.\n",
    "\n",
    "        Loads a dataset consisting of a CSV and names file.  The names file\n",
    "        describes the CSV, and its specification is found in another module.\n",
    "\n",
    "        A dataset is suitable for either classification or regression tasks.\n",
    "        \"\"\"\n",
    "\n",
    "        # If the attribute and class names are not present, then they must be\n",
    "        # read from the names file\n",
    "        if self.attributeNames is None or self.classNames is None:\n",
    "            names = namesFileParser(namesFilename)\n",
    "            self.attributeNames = names.attributes\n",
    "            self.classNames = names.classes\n",
    "            self.regression = names.regression\n",
    "\n",
    "        # load data\n",
    "        data = self.readCSV(dataFilename)\n",
    "\n",
    "        # Set the category values for discrete attributes according to the names\n",
    "        for att in self.attributeNames:\n",
    "\n",
    "            if att['values'] == 'continuous' or att['values'] == 'ignore':\n",
    "                continue\n",
    "\n",
    "            attributeName = att['name']\n",
    "\n",
    "            data[attributeName].cat.set_categories(att['values'], inplace=True)\n",
    "\n",
    "        # Categorical data has had its missing values filled in with NaNs.\n",
    "        # These should be put back to question marks\n",
    "        for col in data.select_dtypes(include=['category']).columns:\n",
    "            data[col].fillna(value='?', inplace=True)\n",
    "\n",
    "        # Record the classes and attributes\n",
    "        if self.hasClassLabels:\n",
    "            self.classes = data['target']\n",
    "            data.drop('target', axis=1, inplace=True)\n",
    "\n",
    "        # We are now done with the dataset\n",
    "        self.data = data\n",
    "\n",
    "    def readCSV(self, dataFilename):\n",
    "        \"\"\"Perform the actual loading of the CSV into dataframe.\n",
    "\n",
    "        Loads data from disk into a pandas dataframe.  The dataframe must\n",
    "        be supplied with several options in order to successfully read the\n",
    "        CSV.  These options are described by other functions.\n",
    "\n",
    "        There are two modes of operation depending on whether or not class\n",
    "        labels are included with the data.  For training, there must be a\n",
    "        target variable.  Testing however does not require this.  If data\n",
    "        fails to be read with a class label, it will retry reading without\n",
    "        the class label.\n",
    "\n",
    "        Args:\n",
    "            dataFilename (str): The CSV file to be read.\n",
    "\n",
    "        Returns:\n",
    "            The raw pandas dataframe\n",
    "        \"\"\"\n",
    "\n",
    "        # try to load the data with the class labels\n",
    "        try:\n",
    "            data = pd.read_csv(\n",
    "                self.findFirstFile(dataFilename, ['gz', 'bz2', 'xz', None]),\n",
    "                header=None,\n",
    "                names=[x['name'] for x in self.attributeNames] + ['target'],\n",
    "                dtype=self.getColumnTypes(),\n",
    "                na_values='?',\n",
    "                usecols=self.getUsableColumns()\n",
    "            )\n",
    "\n",
    "            self.hasClassLabels = True\n",
    "\n",
    "            return data\n",
    "\n",
    "        except pd.io.common.CParserError:\n",
    "            pass\n",
    "\n",
    "        # try to load the data without the class labels\n",
    "        data = pd.read_csv(\n",
    "            self.findFirstFile(dataFilename, ['gz', 'bz2', 'xz', None]),\n",
    "            header=None,\n",
    "            names=[x['name'] for x in self.attributeNames],\n",
    "            dtype=self.getColumnTypes(readClass=False),\n",
    "            na_values='?',\n",
    "            usecols=self.getUsableColumns(readClass=False)\n",
    "        )\n",
    "        self.hasClassLabels = False\n",
    "        return data\n",
    "\n",
    "    def getUsableColumns(self, readClass=True):\n",
    "        \"\"\"Returns the column names to read into dataframe.\n",
    "\n",
    "        If a column is marked ignore in the names file, it can be excluded\n",
    "        from ever being read into the dataframe.  This function returns\n",
    "        the names of all columns not marked ignore.\n",
    "\n",
    "        As test datasets do not strictly require class labels, the decision\n",
    "        to read the class variable is controlled by the readClass argument.\n",
    "\n",
    "        Args:\n",
    "            readClass (bool): True for reading class label, false otherwise.\n",
    "\n",
    "        Returns:\n",
    "            A list of attribute names\n",
    "        \"\"\"\n",
    "\n",
    "        # Do not read columns marked ignore\n",
    "        useCols = list()\n",
    "        for attribute in self.attributeNames:\n",
    "            if attribute['values'] != 'ignore':\n",
    "                useCols.append(attribute['name'])\n",
    "\n",
    "        # Class labels are not strictly necessary\n",
    "        if readClass:\n",
    "            useCols.append('target')\n",
    "\n",
    "        return useCols\n",
    "\n",
    "    def getColumnTypes(self, readClass=True):\n",
    "        \"\"\"Returns a mapping of attribute name to attribute type.\n",
    "\n",
    "        Pandas can assign the data type of features upon reading them in.\n",
    "        This function is used to calculate and assign the feature type based\n",
    "        upon the information in the names file.\n",
    "\n",
    "        As test datasets do not strictly require class labels, the decision\n",
    "        to read the class variable is controlled by the readClass argument.\n",
    "\n",
    "        Args:\n",
    "            readClass (bool): True for reading class label, false otherwise.\n",
    "\n",
    "        Returns:\n",
    "            A dict indexed by attribute name and containing the pandas column\n",
    "            type category.\n",
    "        \"\"\"\n",
    "        # Read the attribute type and assign the column type based on\n",
    "        # discrete or continuous type.  Do not assign anything for columns\n",
    "        # marked ignore\n",
    "        dtypeSpecifier = dict()\n",
    "        for attribute in self.attributeNames:\n",
    "            attributeName = attribute['name']\n",
    "            if attribute['values'] == 'continuous':\n",
    "                dtypeSpecifier[attributeName] = np.float64\n",
    "            elif attribute['values'] != 'ignore':\n",
    "                dtypeSpecifier[attributeName] = 'category'\n",
    "\n",
    "        # Read the class label and assign the column type based on regression\n",
    "        # or classification\n",
    "        if readClass:\n",
    "            if self.regression:\n",
    "                dtypeSpecifier['target'] = np.float64\n",
    "            else:\n",
    "                dtypeSpecifier['target'] = str\n",
    "\n",
    "        return dtypeSpecifier\n",
    "\n",
    "    @staticmethod\n",
    "    def findFirstFile(filestem, extensions):\n",
    "        \"\"\"Finds the first file of the name filestem.[extensions]\n",
    "\n",
    "        Searches disk for a file beginning with the prefix `filestem`, followed\n",
    "        by a period, and succeeded by one of the extensions in the argument\n",
    "        extensions.  The extensions are searched in the order they are \n",
    "        presented\n",
    "\n",
    "        Args:\n",
    "            extensions (list(str)): A list of file extensions to search through\n",
    "\n",
    "        Returns:\n",
    "            The name of the file\n",
    "        \"\"\"\n",
    "        for ext in extensions:\n",
    "            if ext is not None:\n",
    "                if os.path.exists(filestem + '.' + ext):\n",
    "                    return filestem + '.' + ext\n",
    "            else:\n",
    "                if os.path.exists(filestem):\n",
    "                    return filestem\n",
    "        return filestem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    trainFilename = './krk/krk.data'\n",
    "namesFilename = './krk/krk.names'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSet = dataset(trainFilename, namesFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0', '_c1', '_c2', '_c3', '_c4', '_c5', '_c6']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingSet.data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-49-b64e61078a89>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-49-b64e61078a89>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    df = reduce(lambda trainingSet.data, idx: trainingSet.data.withColumnRenamed(oldColumns[idx], newColumns[idx]), xrange(len(oldColumns)), trainingSet.data)\u001b[0m\n\u001b[0m                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "df = reduce(lambda trainingSet.data, idx: trainingSet.data.withColumnRenamed(oldColumns[idx], newColumns[idx]), xrange(len(oldColumns)), trainingSet.data)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = trainingSet.data.toDF(*(temp_dict[c] for c in trainingSet.data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------------------+---------------+---------------+---------------+---------------+------+\n",
      "|white king file (col)|white king rank (row)|white rook file|white rook rank|black king file|blank king rank|target|\n",
      "+---------------------+---------------------+---------------+---------------+---------------+---------------+------+\n",
      "|                    a|                    1|              b|              3|              c|              2|  draw|\n",
      "|                    a|                    1|              c|              1|              c|              2|  draw|\n",
      "|                    a|                    1|              c|              1|              d|              1|  draw|\n",
      "|                    a|                    1|              c|              1|              d|              2|  draw|\n",
      "|                    a|                    1|              c|              2|              c|              1|  draw|\n",
      "|                    a|                    1|              c|              2|              c|              3|  draw|\n",
      "|                    a|                    1|              c|              2|              d|              1|  draw|\n",
      "|                    a|                    1|              c|              2|              d|              2|  draw|\n",
      "|                    a|                    1|              c|              2|              d|              3|  draw|\n",
      "|                    a|                    1|              c|              3|              c|              2|  draw|\n",
      "|                    a|                    1|              c|              3|              d|              2|  draw|\n",
      "|                    a|                    1|              c|              3|              d|              3|  draw|\n",
      "|                    a|                    1|              c|              3|              d|              4|  draw|\n",
      "|                    a|                    1|              c|              4|              d|              3|  draw|\n",
      "|                    a|                    1|              d|              1|              c|              1|  draw|\n",
      "|                    a|                    1|              d|              1|              c|              2|  draw|\n",
      "|                    a|                    1|              d|              1|              d|              2|  draw|\n",
      "|                    a|                    1|              d|              1|              e|              1|  draw|\n",
      "|                    a|                    1|              d|              1|              e|              2|  draw|\n",
      "|                    a|                    1|              d|              2|              c|              1|  draw|\n",
      "+---------------------+---------------------+---------------+---------------+---------------+---------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, VectorIndexer\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------------------+---------------+---------------+---------------+---------------+------+-----------------------------+----------------------------+\n",
      "|white king file (col)|white king rank (row)|white rook file|white rook rank|black king file|blank king rank|target|white king file (col) numeric|white king file (col) vector|\n",
      "+---------------------+---------------------+---------------+---------------+---------------+---------------+------+-----------------------------+----------------------------+\n",
      "|                    a|                    1|              b|              3|              c|              2|  draw|                          3.0|                   (3,[],[])|\n",
      "|                    a|                    1|              c|              1|              c|              2|  draw|                          3.0|                   (3,[],[])|\n",
      "|                    a|                    1|              c|              1|              d|              1|  draw|                          3.0|                   (3,[],[])|\n",
      "|                    a|                    1|              c|              1|              d|              2|  draw|                          3.0|                   (3,[],[])|\n",
      "|                    a|                    1|              c|              2|              c|              1|  draw|                          3.0|                   (3,[],[])|\n",
      "|                    a|                    1|              c|              2|              c|              3|  draw|                          3.0|                   (3,[],[])|\n",
      "|                    a|                    1|              c|              2|              d|              1|  draw|                          3.0|                   (3,[],[])|\n",
      "|                    a|                    1|              c|              2|              d|              2|  draw|                          3.0|                   (3,[],[])|\n",
      "|                    a|                    1|              c|              2|              d|              3|  draw|                          3.0|                   (3,[],[])|\n",
      "|                    a|                    1|              c|              3|              c|              2|  draw|                          3.0|                   (3,[],[])|\n",
      "|                    a|                    1|              c|              3|              d|              2|  draw|                          3.0|                   (3,[],[])|\n",
      "|                    a|                    1|              c|              3|              d|              3|  draw|                          3.0|                   (3,[],[])|\n",
      "|                    a|                    1|              c|              3|              d|              4|  draw|                          3.0|                   (3,[],[])|\n",
      "|                    a|                    1|              c|              4|              d|              3|  draw|                          3.0|                   (3,[],[])|\n",
      "|                    a|                    1|              d|              1|              c|              1|  draw|                          3.0|                   (3,[],[])|\n",
      "|                    a|                    1|              d|              1|              c|              2|  draw|                          3.0|                   (3,[],[])|\n",
      "|                    a|                    1|              d|              1|              d|              2|  draw|                          3.0|                   (3,[],[])|\n",
      "|                    a|                    1|              d|              1|              e|              1|  draw|                          3.0|                   (3,[],[])|\n",
      "|                    a|                    1|              d|              1|              e|              2|  draw|                          3.0|                   (3,[],[])|\n",
      "|                    a|                    1|              d|              2|              c|              1|  draw|                          3.0|                   (3,[],[])|\n",
      "+---------------------+---------------------+---------------+---------------+---------------+---------------+------+-----------------------------+----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexer = StringIndexer(inputCol=\"white king file (col)\", outputCol=\"white king file (col) numeric\").fit(data)\n",
    "encoder = OneHotEncoder(inputCol=\"white king file (col) numeric\", outputCol=\"white king file (col) vector\")\n",
    "assembler = VectorAssembler(inputCols=[\"white king file (col) vector\", \"white king rank (row)\", \"white rook rank\"], outputCol=\"features\")\n",
    "pipeline = Pipeline(stages=[indexer, encoder])\n",
    "model = pipeline.fit(data)\n",
    "transformed = model.transform(data)\n",
    "transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'10'>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "lit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.sql.functions.lit.\n: java.lang.RuntimeException: Unsupported literal type class java.util.ArrayList [a, b, c, d, ?]\n\tat org.apache.spark.sql.catalyst.expressions.Literal$.apply(literals.scala:77)\n\tat org.apache.spark.sql.catalyst.expressions.Literal$$anonfun$create$2.apply(literals.scala:163)\n\tat org.apache.spark.sql.catalyst.expressions.Literal$$anonfun$create$2.apply(literals.scala:163)\n\tat scala.util.Try.getOrElse(Try.scala:79)\n\tat org.apache.spark.sql.catalyst.expressions.Literal$.create(literals.scala:162)\n\tat org.apache.spark.sql.functions$.typedLit(functions.scala:112)\n\tat org.apache.spark.sql.functions$.lit(functions.scala:95)\n\tat org.apache.spark.sql.functions.lit(functions.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-f366c3273991>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#     data[attributeName].cat.set_categories(att['values'], inplace=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mnew_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattributeName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'values'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/functions.py\u001b[0m in \u001b[0;36m_\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.sql.functions.lit.\n: java.lang.RuntimeException: Unsupported literal type class java.util.ArrayList [a, b, c, d, ?]\n\tat org.apache.spark.sql.catalyst.expressions.Literal$.apply(literals.scala:77)\n\tat org.apache.spark.sql.catalyst.expressions.Literal$$anonfun$create$2.apply(literals.scala:163)\n\tat org.apache.spark.sql.catalyst.expressions.Literal$$anonfun$create$2.apply(literals.scala:163)\n\tat scala.util.Try.getOrElse(Try.scala:79)\n\tat org.apache.spark.sql.catalyst.expressions.Literal$.create(literals.scala:162)\n\tat org.apache.spark.sql.functions$.typedLit(functions.scala:112)\n\tat org.apache.spark.sql.functions$.lit(functions.scala:95)\n\tat org.apache.spark.sql.functions.lit(functions.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "for att in attributeNames:\n",
    "    if att['values'] == 'continuous' or att['values'] == 'ignore':\n",
    "        continue\n",
    "    \n",
    "    attributeName = att['name']\n",
    "\n",
    "#     data[attributeName].cat.set_categories(att['values'], inplace=True)\n",
    "    new_df = data.withColumn(attributeName, lit(att['values']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/flask-genesis/genesis/dataset.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from names import namesFileParser\n",
    "\n",
    "class dataset(object):\n",
    "    \"\"\"Representation of a dataset\n",
    "\n",
    "    Each row contains a feature vector.  Continuous and discrete features are\n",
    "    handled within the vector.  The class labels may also be discrete or \n",
    "    continuous, representative of classification or regression, respectively.\n",
    "\n",
    "    The dataset is read in by processing a CSV file and a .names file\n",
    "    describing the CSV file contains.  Alternatively, manual specification of\n",
    "    the .names file contents through arguments to __init__ may be provided.\n",
    "\n",
    "    Feature vectors containing missing data is also handled.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataFilename, namesFilename=None, attributeNames=None, classNames=None):\n",
    "\n",
    "        self.hasClassLabels = False  # Not all datasets must have class labels (test set)\n",
    "        self.attributeNames = attributeNames  # List of attribute names\n",
    "        self.classNames = classNames  # List of class names\n",
    "        self.classes = None  # Dataframe of class labels\n",
    "        self.data = None  # Dataframe of dataset\n",
    "\n",
    "        # Determine if this is a regression problem\n",
    "        self.regression = namesFileParser.isRegression(self.classNames)\n",
    "\n",
    "        self.readData(dataFilename, namesFilename)\n",
    "\n",
    "    def readData(self, dataFilename, namesFilename):\n",
    "        \"\"\"Loads a dataset into memory.\n",
    "\n",
    "        Loads a dataset consisting of a CSV and names file.  The names file\n",
    "        describes the CSV, and its specification is found in another module.\n",
    "\n",
    "        A dataset is suitable for either classification or regression tasks.\n",
    "        \"\"\"\n",
    "\n",
    "        # If the attribute and class names are not present, then they must be\n",
    "        # read from the names file\n",
    "        if self.attributeNames is None or self.classNames is None:\n",
    "            names = namesFileParser(namesFilename)\n",
    "            self.attributeNames = names.attributes\n",
    "            self.classNames = names.classes\n",
    "            self.regression = names.regression\n",
    "\n",
    "        # load data\n",
    "        data = self.readCSV(dataFilename)\n",
    "\n",
    "        # Set the category values for discrete attributes according to the names\n",
    "#         for att in self.attributeNames:\n",
    "\n",
    "#             if att['values'] == 'continuous' or att['values'] == 'ignore':\n",
    "#                 continue\n",
    "\n",
    "#             attributeName = att['name']\n",
    "\n",
    "#             data[attributeName].cat.set_categories(att['values'], inplace=True)\n",
    "\n",
    "#         # Categorical data has had its missing values filled in with NaNs.\n",
    "#         # These should be put back to question marks\n",
    "#         for col in data.select_dtypes(include=['category']).columns:\n",
    "#             data[col].fillna(value='?', inplace=True)\n",
    "\n",
    "#         # Record the classes and attributes\n",
    "#         if self.hasClassLabels:\n",
    "#             self.classes = data['target']\n",
    "#             data.drop('target', axis=1, inplace=True)\n",
    "\n",
    "        # We are now done with the dataset\n",
    "        self.data = data\n",
    "\n",
    "    def readCSV(self, dataFilename):\n",
    "        \"\"\"Perform the actual loading of the CSV into dataframe.\n",
    "\n",
    "        Loads data from disk into a pandas dataframe.  The dataframe must\n",
    "        be supplied with several options in order to successfully read the\n",
    "        CSV.  These options are described by other functions.\n",
    "\n",
    "        There are two modes of operation depending on whether or not class\n",
    "        labels are included with the data.  For training, there must be a\n",
    "        target variable.  Testing however does not require this.  If data\n",
    "        fails to be read with a class label, it will retry reading without\n",
    "        the class label.\n",
    "\n",
    "        Args:\n",
    "            dataFilename (str): The CSV file to be read.\n",
    "\n",
    "        Returns:\n",
    "            The raw pandas dataframe\n",
    "        \"\"\"\n",
    "\n",
    "        # try to load the data with the class labels\n",
    "        try:\n",
    "#             data = pd.read_csv(\n",
    "#                 self.findFirstFile(dataFilename, ['gz', 'bz2', 'xz', None]),\n",
    "#                 header=None,\n",
    "#                 names=[x['name'] for x in self.attributeNames] + ['target'],\n",
    "#                 dtype=self.getColumnTypes(),\n",
    "#                 na_values='?',\n",
    "#                 usecols=self.getUsableColumns()\n",
    "#             )\n",
    "\n",
    "            data = spark.read.option(\"header\", None).option(\"names\", [x['name'] for x in self.attributeNames] + ['target']).option(\"dtype\", self.getColumnTypes()).option(\"na_values\", \"?\").option(\"usecols\", self.getUsableColumns()).csv(trainFilename)\n",
    "\n",
    "            self.hasClassLabels = True\n",
    "\n",
    "            return data\n",
    "\n",
    "        except pd.io.common.CParserError:\n",
    "            pass\n",
    "\n",
    "        # try to load the data without the class labels\n",
    "#         data = pd.read_csv(\n",
    "#             self.findFirstFile(dataFilename, ['gz', 'bz2', 'xz', None]),\n",
    "#             header=None,\n",
    "#             names=[x['name'] for x in self.attributeNames],\n",
    "#             dtype=self.getColumnTypes(readClass=False),\n",
    "#             na_values='?',\n",
    "#             usecols=self.getUsableColumns(readClass=False)\n",
    "#         )\n",
    "\n",
    "        data = spark.read.option(\"header\", None).option(\"names\", [x['name'] for x in self.attributeNames]).option(\"dtype\", self.getColumnTypes()).option(\"na_values\", \"?\").option(\"usecols\", self.getUsableColumns()).csv(trainFilename)\n",
    "        \n",
    "        self.hasClassLabels = False\n",
    "        return data\n",
    "\n",
    "    def getUsableColumns(self, readClass=True):\n",
    "        \"\"\"Returns the column names to read into dataframe.\n",
    "\n",
    "        If a column is marked ignore in the names file, it can be excluded\n",
    "        from ever being read into the dataframe.  This function returns\n",
    "        the names of all columns not marked ignore.\n",
    "\n",
    "        As test datasets do not strictly require class labels, the decision\n",
    "        to read the class variable is controlled by the readClass argument.\n",
    "\n",
    "        Args:\n",
    "            readClass (bool): True for reading class label, false otherwise.\n",
    "\n",
    "        Returns:\n",
    "            A list of attribute names\n",
    "        \"\"\"\n",
    "\n",
    "        # Do not read columns marked ignore\n",
    "        useCols = list()\n",
    "        for attribute in self.attributeNames:\n",
    "            if attribute['values'] != 'ignore':\n",
    "                useCols.append(attribute['name'])\n",
    "\n",
    "        # Class labels are not strictly necessary\n",
    "        if readClass:\n",
    "            useCols.append('target')\n",
    "\n",
    "        return useCols\n",
    "\n",
    "    def getColumnTypes(self, readClass=True):\n",
    "        \"\"\"Returns a mapping of attribute name to attribute type.\n",
    "\n",
    "        Pandas can assign the data type of features upon reading them in.\n",
    "        This function is used to calculate and assign the feature type based\n",
    "        upon the information in the names file.\n",
    "\n",
    "        As test datasets do not strictly require class labels, the decision\n",
    "        to read the class variable is controlled by the readClass argument.\n",
    "\n",
    "        Args:\n",
    "            readClass (bool): True for reading class label, false otherwise.\n",
    "\n",
    "        Returns:\n",
    "            A dict indexed by attribute name and containing the pandas column\n",
    "            type category.\n",
    "        \"\"\"\n",
    "        # Read the attribute type and assign the column type based on\n",
    "        # discrete or continuous type.  Do not assign anything for columns\n",
    "        # marked ignore\n",
    "        dtypeSpecifier = dict()\n",
    "        for attribute in self.attributeNames:\n",
    "            attributeName = attribute['name']\n",
    "            if attribute['values'] == 'continuous':\n",
    "                dtypeSpecifier[attributeName] = np.float64\n",
    "            elif attribute['values'] != 'ignore':\n",
    "                dtypeSpecifier[attributeName] = 'category'\n",
    "\n",
    "        # Read the class label and assign the column type based on regression\n",
    "        # or classification\n",
    "        if readClass:\n",
    "            if self.regression:\n",
    "                dtypeSpecifier['target'] = np.float64\n",
    "            else:\n",
    "                dtypeSpecifier['target'] = str\n",
    "\n",
    "        return dtypeSpecifier\n",
    "\n",
    "    @staticmethod\n",
    "    def findFirstFile(filestem, extensions):\n",
    "        \"\"\"Finds the first file of the name filestem.[extensions]\n",
    "\n",
    "        Searches disk for a file beginning with the prefix `filestem`, followed\n",
    "        by a period, and succeeded by one of the extensions in the argument\n",
    "        extensions.  The extensions are searched in the order they are \n",
    "        presented\n",
    "\n",
    "        Args:\n",
    "            extensions (list(str)): A list of file extensions to search through\n",
    "\n",
    "        Returns:\n",
    "            The name of the file\n",
    "        \"\"\"\n",
    "        for ext in extensions:\n",
    "            if ext is not None:\n",
    "                if os.path.exists(filestem + '.' + ext):\n",
    "                    return filestem + '.' + ext\n",
    "            else:\n",
    "                if os.path.exists(filestem):\n",
    "                    return filestem\n",
    "        return filestem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark_df = spark.read.option(\"header\", None).csv(trainFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = namesFileParser(namesFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributeNames = names.attributes\n",
    "classNames = names.classes\n",
    "regression = names.regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_names = []\n",
    "for att in attributeNames:\n",
    "    temp_names.append(att['name'])\n",
    "temp_names.append('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'white king file (col)', 'values': ['a', 'b', 'c', 'd', '?']},\n",
       " {'name': 'white king rank (row)', 'values': ['1', '2', '3', '4', '?']},\n",
       " {'name': 'white rook file',\n",
       "  'values': ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', '?']},\n",
       " {'name': 'white rook rank',\n",
       "  'values': ['1', '2', '3', '4', '5', '6', '7', '8', '?']},\n",
       " {'name': 'black king file',\n",
       "  'values': ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', '?']},\n",
       " {'name': 'blank king rank',\n",
       "  'values': ['1', '2', '3', '4', '5', '6', '7', '8', '?']}]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributeNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "newColumns = temp_names\n",
    "oldColumns = trainingSet.data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "newColumns = newColumns.append('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['white king file (col)',\n",
       " 'white king rank (row)',\n",
       " 'white rook file',\n",
       " 'white rook rank',\n",
       " 'black king file',\n",
       " 'blank king rank',\n",
       " 'target']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dict = dict(zip(oldColumns, newColumns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'A' : np.random.randn(9), 'B' : pd.Series(list('aabbcd ab')).astype('category')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.643247</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.840610</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.072254</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.341142</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.031222</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.388554</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.860910</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.222105</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.570079</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A  B\n",
       "0  1.643247  a\n",
       "1 -0.840610  a\n",
       "2 -0.072254  b\n",
       "3  0.341142  b\n",
       "4 -1.031222  c\n",
       "5  1.388554  d\n",
       "6  0.860910   \n",
       "7  2.222105  a\n",
       "8  1.570079  b"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      a\n",
       "1      a\n",
       "2      b\n",
       "3      b\n",
       "4      c\n",
       "5      d\n",
       "6    NaN\n",
       "7      a\n",
       "8      b\n",
       "dtype: category\n",
       "Categories (5, object): [a, b, c, d, ?]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.B.cat.set_categories(['a', 'b', 'c', 'd', '?'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
